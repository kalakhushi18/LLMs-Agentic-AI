# Prompt Engineering

In addition to Retrieval-Augmented Generation (RAG), prompt engineering is one of the key techniques essential for building effective LLM systems.

What is prompt engineering?

**Prompt engineering** is the process of crafting the prompt with effective instructions for a model, such that it consistently generates answers that fulfill the given task. Personally, I see it as communicating to a model what we want from it.

Now, let's get to know the major tools and techniques available to engineer prompts.

## Message roles and instruction following

We can provide instructions to the model with differing levels of authority using message roles. Here is an example from OpenAI.

```python
from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4.1",
    messages=[
        {
            "role": "developer",
            "content": "Talk like a pirate."
        },
        {
            "role": "user",
            "content": "How are you today?"
        }
    ]
)

print(completion.choices[0].message.content)
```

The [OpenAI model spec](https://model-spec.openai.com/2025-02-12.html#chain_of_command) describes how their models give different levels of priority to messages with different roles. (Sidenote) At least they were trained to do it and some evals have shown that they took this training by heart and give different levels of priority.
| Role | Description |
|------------|-----------------------------------------------------------------------------|
| Developer | Messages are instructions provided by the application developer, prioritized ahead of user messages. |
| User | Messages are instructions provided by an end user, prioritized behind developer messages. |
| Assistant | Messages generated by the model have the assistant role. |
(taken from [OpenAIs Docs](https://platform.openai.com/docs/guides/text?api-mode=chat))

The only part we typically have control over is the developer message—usually the first message in the messages list. This is where we begin our prompt engineering from a builder’s perspective. The same principles apply when we act as users of a model, but since we are designing LLM systems for others, the developer message is the primary point of influence we truly control.

**Note:** In other models, the developer message may be referred to as the system message or by another name, but the purpose remains the same across implementations.

In general, such a (developer) message will contain the following parts, usually in this order (though the exact optimal content and order may vary by which model we are using):

- **Identity/Role:** Describe the purpose, communication style, and high-level goals of the assistant.
- **Instructions:** Provide guidance to the model on how to generate the response you want. What rules should it follow? What should the model do, and what should the model never do? This section could contain many subsections as relevant for your use case, like how the model should call custom functions.
- **Examples:** Provide examples of possible inputs, along with the desired output from the model.
- **Context:** Give the model any additional information it might need to generate a response, like private/proprietary data outside its training data, or any other data you know will be particularly relevant. This content is usually best positioned near the end of your prompt, as you may include different context for different generation requests.

Below is an example of using Markdown and XML tags to construct a developer message with distinct sections and supporting examples.(taken from [OpenAIs Docs](https://platform.openai.com/docs/guides/text?api-mode=chat)). Just as we humans prefer well-structured text with spaces, newlines, paragraphs, and headings for easier readability, language models also benefit from clear formatting. Providing proper structure improves their ability to interpret and follow instructions. This is typically achieved using Markdown and/or XML tags, as shown below.

```text
# Identity

You are coding assistant that helps enforce the use of snake case
variables in JavaScript code, and writing code that will run in
Internet Explorer version 6.

# Instructions

* When defining variables, use snake case names (e.g. my_variable)
  instead of camel case names (e.g. myVariable).
* To support old browsers, declare variables using the older
  "var" keyword.
* Do not give responses with Markdown formatting, just return
  the code as requested.

# Examples

<user_query>
How do I declare a string variable for a first name?
</user_query>

<assistant_response>
var first_name = "Anna";
</assistant_response>
```

As we can see **Prompt Engineering** is no rocket science or dark magic. It is basically just solid communication. As with humans it helps to clearly communicate what we want, give them all the necessary context and put some examples for clarification on top. Done!

Sounded now simpler as it is. But as in human communication the complexity of the task changes how difficult it is to explain it. That is of course also the case for LLMs. The example above involves a very simple task that is easy to explain and leaves little room for ambiguity. But what if the task were for the AI model to act as the user’s best friend? That would be enormously complex and inherently ambiguous.

So for complexer tasks we have to think of how to communicate them correctly and that is actually a quite hard task. We already learned that we can steer the model with the Identity, Instructions (these we can always make more precise and comprehensive) and Examples into the right directions. For the latter we have done something what is called _One-Shot-Learning_ i.e. giving the model one example in the prompt to learn from how to behave. In contrast not giving a single example is called _Zero-Shot-Learning_. As we move to more complex task scenarios using more examples makes sense (at least you probably had more than one example exercise for your exams :)) This is called **Few-Shot-Learning**.

The model implicitly "picks up" the pattern from those examples and applies it to generate the answer. When creating examples, we try to show a diverse range of possible inputs with the desired outputs and if necesssary also describe when which example is the most favorable.

Typically, we will provide these examples as part of a developer message.
Here's an example developer message containing examples that show a model how to classify positive or negative customer service reviews.(taken from [OpenAIs Docs](https://platform.openai.com/docs/guides/text?api-mode=chat))

```text
# Identity

You are a helpful assistant that labels short product reviews as
Positive, Negative, or Neutral.

# Instructions

* Only output a single word in your response with no additional formatting
  or commentary.
* Your response should only be one of the words "Positive", "Negative", or
  "Neutral" depending on the sentiment of the product review you are given.

# Examples

<product_review id="example-1">
I absolutely love this headphones — sound quality is amazing!
</product_review>

<assistant_response id="example-1">
Positive
</assistant_response>

<product_review id="example-2">
Battery life is okay, but the ear pads feel cheap.
</product_review>

<assistant_response id="example-2">
Neutral
</assistant_response>

<product_review id="example-3">
Terrible customer service, I'll never buy from them again.
</product_review>

<assistant_response id="example-3">
Negative
</assistant_response>
```

The key challenge here lies in defining where we—or in this case, OpenAI—draw the line between what is considered positive, neutral, or negative. This distinction is difficult to convey to a model through instructions alone, which is why providing examples becomes an effective approach. It should be clear that the quality and diversity of these examples are especially important. In my opinion, OpenAI has not done a great job with this particular example. Can you think of why? (See the explanation below.)

**Solution**
In the example above, OpenAI provided three examples—one for each class. However, the real challenge isn’t in identifying the clear-cut positive or negative cases, which makes those examples of limited value in my opinion. What truly matters are the edge cases—the subtle distinctions between _positive and neutral, and neutral and negative._ Yet, no examples are provided for those boundaries.

Even the "neutral" example given felt slightly negative to me on a personal level. This highlights another important aspect: when offering examples, it often helps to include a brief explanation of why a particular label was chosen. Remember, LLMs are best thought of as coworkers—we need to explain the task to them clearly, just as we would to a human teammate.

Fortunately, OpenAI’s approach serves as a valuable case study in how _not_ to handle such instruction. Below, I’ve included additional examples that I believe better illustrate where the line lies—specifically between positive and neutral—along with explanations. _My general approach is always to ask: If I were given this task without further context, what would I need in order to do it well?_

```text
<product_review id="example-4">
The headphones look great and are comfortable, but the sound isn't much better than cheaper options.
</product_review>

<assistant_response id="example-4">
Neutral
</assistant_response>
<explanation id="example-4">
The review includes both praise (appearance and comfort) and criticism (sound quality), resulting in a balanced sentiment.
</explanation>

<product_review id="example-5">
They work fine for casual use, and the design is sleek. Not blown away, but satisfied.
</product_review>

<assistant_response id="example-5">
Positive
</assistant_response>
<explanation id="example-5">
The reviewer expresses overall satisfaction despite lacking enthusiasm, tipping the sentiment slightly into positive territory.
</explanation>

<product_review id="example-6">
Setup was a bit tricky, but once working, the performance is decent.
</product_review>

<assistant_response id="example-6">
Neutral
</assistant_response>
<explanation id="example-6">
While the final outcome is acceptable, the initial frustration with setup tempers the overall sentiment, keeping it neutral.
</explanation>

<product_review id="example-7">
The build feels solid, and though the features are limited, they do exactly what I need.
</product_review>

<assistant_response id="example-7">
Positive
</assistant_response>
<explanation id="example-7">
Despite limited features, the reviewer finds the product well-suited to their needs, making the overall tone positive.
</explanation>
```

## Include **all!** relevant context information

The problem of communicating with AI is that we often do not think about all the things the AI has to know to properly do the task. To navigate that it is often helpful to think of the AI as of a new employee who brings some general knowledge and skills, but has no plan or further information hence we have to give him/her all the detail in complete what is not necessary in our normal communication because there is a lot of inherent knowledge already shared between us, so we just know what the other person means when she/he says this. This is not the case with LLMs (at least yet -2025), so we have to painfully describe every detail.
This is why it is really important to give the model all the necessary context.

Here are a few common reasons why you might do this:

- To give the model access to proprietary data, or any other data outside the data set the model was trained on (mainly material on the Internet).
- To constrain the model's response to a specific set of resources that you have determined will be most beneficial.

If part or all of this context needs to be set dynamically, we learned in RAG module, how we can do that.

### Additional articles

[Best practices for prompt engineering with the OpenAI API | OpenAI Help Center](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api)